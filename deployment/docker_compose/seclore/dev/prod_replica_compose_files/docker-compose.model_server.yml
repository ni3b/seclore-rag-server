services:
  inference_model_server:
    image: git.seclore.com/automation/onyx-model-server:${IMAGE_TAG:-latest}
    command: >
      /bin/sh -c "if [ \"${DISABLE_MODEL_SERVER:-false}\" = \"True\" ]; then
        echo 'Skipping service...';
        exit 0;
      else
        exec uvicorn model_server.main:app --host 0.0.0.0 --port 9000;
      fi"
    restart: on-failure
    environment:
      - MIN_THREADS_ML_MODELS=${MIN_THREADS_ML_MODELS:-}
      # Set to debug to get more fine-grained logs
      - LOG_LEVEL=${LOG_LEVEL:-info}

      # Analytics Configs
      - SENTRY_DSN=${SENTRY_DSN:-}
    volumes:
      # Not necessary, this is just to reduce download time during startup
      - model_cache_huggingface:/root/.cache/huggingface/
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

  indexing_model_server:
    image: git.seclore.com/automation/onyx-model-server:${IMAGE_TAG:-latest}
    command: >
      /bin/sh -c "if [ \"${DISABLE_MODEL_SERVER:-false}\" = \"True\" ]; then
        echo 'Skipping service...';
        exit 0;
      else
        exec uvicorn model_server.main:app --host 0.0.0.0 --port 9000;
      fi"
    restart: on-failure
    environment:
      - INDEX_BATCH_SIZE=${INDEX_BATCH_SIZE:-}
      - MIN_THREADS_ML_MODELS=${MIN_THREADS_ML_MODELS:-}
      - INDEXING_ONLY=True
      # Set to debug to get more fine-grained logs
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - CLIENT_EMBEDDING_TIMEOUT=${CLIENT_EMBEDDING_TIMEOUT:-}

      # Analytics Configs
      - SENTRY_DSN=${SENTRY_DSN:-}
    volumes:
      # Not necessary, this is just to reduce download time during startup
      - indexing_huggingface_model_cache:/root/.cache/huggingface/
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

  image_model_server:
    image: git.seclore.com/automation/onyx-image-model-server:${IMAGE_TAG:-latest}
    build:
      context: ../../../../backend
      dockerfile: Dockerfile.image_model_server
    command: >
      /bin/sh -c "if [ \"${DISABLE_IMAGE_MODEL_SERVER:-false}\" = \"True\" ]; then
        echo 'Skipping image model server...';
        exit 0;
      else
        exec uvicorn image_model_server.main:app --host 0.0.0.0 --port 9001;
      fi"
    restart: on-failure
    environment:
      - IMAGE_MODEL_SERVER_HOST=0.0.0.0
      - IMAGE_MODEL_SERVER_PORT=9001
      - IMAGE_MODEL_SERVER_TIMEOUT=${IMAGE_MODEL_SERVER_TIMEOUT:-300}
      # Set to debug to get more fine-grained logs
      - LOG_LEVEL=${LOG_LEVEL:-info}
      
      # Claude API configuration (supports both Anthropic and Bedrock)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - CLAUDE_API_KEY=${CLAUDE_API_KEY:-}
      
      # AWS Bedrock configuration (if using Bedrock Claude)
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
      - AWS_REGION_NAME=${AWS_REGION_NAME:-us-east-1}

      # Analytics Configs
      - SENTRY_DSN=${SENTRY_DSN:-}
    volumes:
      # Cache for image processing models
      - image_model_cache_huggingface:/root/.cache/huggingface/
    ports:
      - "9001:9001"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"
        
  # This container name cannot have an underscore in it due to Vespa expectations of the URL
  index:
    image: vespaengine/vespa:8.277.17
    restart: always
    ports:
      - "19071:19071"
      - "8081:8081"
    volumes:
      - vespa_volume:/opt/vespa/var
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

volumes:
  vespa_volume: # Created by the container itself
  model_cache_huggingface:
  indexing_huggingface_model_cache:
  image_model_cache_huggingface:
